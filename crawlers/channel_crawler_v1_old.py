"""
ì±„ë„í†¡ ë¬¸ì„œ í¬ë¡¤ëŸ¬
https://docs.channel.io/help/ko ì—ì„œ ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ í´ë”ë¡œ í¬ë¡¤ë§í•˜ì—¬ Markdownìœ¼ë¡œ ì €ì¥
"""

import os
import re
import time
from pathlib import Path
from typing import Dict, List, Optional, Set
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md


class ChannelDocsCrawler:
    """ì±„ë„í†¡ ë¬¸ì„œ í¬ë¡¤ëŸ¬"""

    def __init__(
        self,
        base_url: str = "https://docs.channel.io/help/ko",
        output_dir: str = "data/domain/saas/channel/docs",
        delay: float = 1.0,
    ):
        """
        Args:
            base_url: í¬ë¡¤ë§í•  ê¸°ë³¸ URL
            output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )
        self.visited_urls: Set[str] = set()

    def sanitize_filename(self, text: str) -> str:
        """íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        text = re.sub(r'[<>:"/\\|?*]', "", text)
        text = re.sub(r"\s+", "_", text.strip())
        # ë„ˆë¬´ ê¸´ íŒŒì¼ëª… ì œí•œ
        if len(text) > 100:
            text = text[:100]
        return text

    def extract_short_category_name(self, full_title: str) -> str:
        """ê¸´ ì¹´í…Œê³ ë¦¬ëª…ì—ì„œ ì§§ì€ ì´ë¦„ ì¶”ì¶œ"""
        # ìˆ˜ë™ ë§¤í•‘ìœ¼ë¡œ ê°„ë‹¨í•œ ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ
        mapping = {
            "ì±„ë„í†¡_ì‹œì‘í•˜ê¸°": "ì±„ë„í†¡_ì‹œì‘í•˜ê¸°",
            "ì±„ë„_ì„¤ì •": "ì±„ë„_ì„¤ì •",
            "ê°œì¸_ì„¤ì •": "ê°œì¸_ì„¤ì •",
            "íŒ€_ë©”ì‹ ì €": "íŒ€_ë©”ì‹ ì €",
            "ê³ ê°_ë©”ì‹ ì €": "ê³ ê°_ë©”ì‹ ì €",
            "ê³ ê°_ì—°ë½ì²˜": "ê³ ê°_ì—°ë½ì²˜",
            "ì›Œí¬í”Œë¡œìš°": "ì›Œí¬í”Œë¡œìš°",
            "ì˜¤í¼ë ˆì´ì…˜": "ì˜¤í¼ë ˆì´ì…˜",
            "ë§ˆì¼€íŒ…": "ë§ˆì¼€íŒ…",
            "ì™¸ë¶€ì„œë¹„ìŠ¤_ì—°ë™": "ì™¸ë¶€ì„œë¹„ìŠ¤_ì—°ë™",
            "AI": "AI",
            "ì•±ìŠ¤í† ì–´": "ì•±ìŠ¤í† ì–´",
            "ë¯¸íŠ¸": "ë¯¸íŠ¸",
            "êµ¬ë…_ë°_ê²°ì œ": "êµ¬ë…_ë°_ê²°ì œ",
            "ë„íë¨¼íŠ¸": "ë„íë¨¼íŠ¸",
        }
        
        # ë§¤í•‘ì—ì„œ ì°¾ê¸°
        for key, value in mapping.items():
            if full_title.startswith(key):
                return value
        
        # ë§¤í•‘ì— ì—†ìœ¼ë©´ "ê°œì˜ ì•„í‹°í´" ì œê±° í›„ ì²« 2ë‹¨ì–´
        text = re.sub(r"\d+ê°œì˜_ì•„í‹°í´.*$", "", full_title)
        parts = text.split("_")
        result = "_".join(parts[:2]) if len(parts) >= 2 else text
        
        return result.strip("_")

    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """URLì—ì„œ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.text, "lxml")
        except Exception as e:
            print(f"âŒ Failed to fetch {url}: {e}")
            return None

    def extract_article_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """í˜ì´ì§€ì—ì„œ ë¬¸ì„œ ë§í¬ ì¶”ì¶œ"""
        links = []
        seen_urls = set()
        
        # ëª¨ë“  ë§í¬ ì°¾ê¸°
        for link_tag in soup.find_all("a", href=True):
            href = link_tag["href"]
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            full_url = urljoin(base_url, href)
            
            # docs.channel.io ë„ë©”ì¸ì˜ help/ko ë§í¬ë§Œ í•„í„°ë§ (articles ë˜ëŠ” categories)
            if (
                "docs.channel.io" in full_url
                and "/help/ko/" in full_url
                and ("/articles/" in full_url or "/categories/" in full_url)
                and full_url not in seen_urls
            ):
                title = link_tag.get_text(strip=True)
                if title and len(title) > 0:
                    # URLì—ì„œ ì¹´í…Œê³ ë¦¬ íƒ€ì… ì¶”ì¶œ
                    category_type = "article" if "/articles/" in full_url else "category"
                    
                    links.append(
                        {
                            "url": full_url,
                            "title": title,
                            "category": category_type,
                            "type": category_type,
                        }
                    )
                    seen_urls.add(full_url)
        
        return links

    def convert_html_to_markdown(self, soup: BeautifulSoup) -> str:
        """HTML ì½˜í…ì¸ ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
        content_selectors = [
            ("article", {}),
            ("div", {"class": "article-content"}),
            ("div", {"class": "content"}),
            ("main", {}),
        ]
        
        content = None
        for tag, attrs in content_selectors:
            content = soup.find(tag, attrs)
            if content:
                break
        
        if not content:
            # ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° body ì „ì²´ ì‚¬ìš©
            content = soup.find("body")
        
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in content.find_all(["script", "style", "nav", "footer", "header"]):
            element.decompose()
        
        # Markdownìœ¼ë¡œ ë³€í™˜
        markdown_text = md(str(content), heading_style="ATX", bullets="*")
        
        # ì—¬ëŸ¬ ì¤„ ê³µë°± ì •ë¦¬
        markdown_text = re.sub(r"\n{3,}", "\n\n", markdown_text)
        
        return markdown_text.strip()

    def save_markdown(
        self, content: str, title: str, category: str, url: str, metadata: Optional[Dict] = None
    ):
        """Markdown íŒŒì¼ë¡œ ì €ì¥"""
        # ì¹´í…Œê³ ë¦¬ëª… ì •ë¦¬ (ë„ˆë¬´ ê¸´ ê²½ìš° ì§§ê²Œ)
        short_category = self.extract_short_category_name(category)
        category_dir = self.output_dir / self.sanitize_filename(short_category)
        category_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        filename = self.sanitize_filename(title) + ".md"
        filepath = category_dir / filename
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        frontmatter = f"""---
title: {title}
category: {category}
source_url: {url}
crawled_at: {time.strftime('%Y-%m-%d %H:%M:%S')}
---

"""
        
        # íŒŒì¼ ì €ì¥
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(frontmatter)
            f.write(f"# {title}\n\n")
            f.write(content)
        
        print(f"âœ… Saved: {filepath.relative_to(self.output_dir)}")

    def crawl_article(self, article_info: Dict[str, str]):
        """ê°œë³„ ë¬¸ì„œ í¬ë¡¤ë§"""
        url = article_info["url"]
        
        # ì´ë¯¸ ë°©ë¬¸í•œ URLì€ ìŠ¤í‚µ
        if url in self.visited_urls:
            return
        
        self.visited_urls.add(url)
        
        print(f"ğŸ“„ Crawling: {article_info['title']} ({url})")
        
        # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.get_page_content(url)
        if not soup:
            return
        
        # ì œëª© ì¶”ì¶œ (h1 íƒœê·¸ ìš°ì„ )
        title_tag = soup.find("h1")
        title = title_tag.get_text(strip=True) if title_tag else article_info["title"]
        
        # HTMLì„ Markdownìœ¼ë¡œ ë³€í™˜
        markdown_content = self.convert_html_to_markdown(soup)
        
        if markdown_content:
            # ì €ì¥
            self.save_markdown(
                content=markdown_content,
                title=title,
                category=article_info["category"],
                url=url,
            )
        
        # ìš”ì²­ ê°„ ëŒ€ê¸°
        time.sleep(self.delay)

    def crawl(self):
        """ì „ì²´ ë¬¸ì„œ í¬ë¡¤ë§ ì‹œì‘"""
        print(f"ğŸš€ Starting crawl from {self.base_url}")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”ì¸ í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("âŒ Failed to fetch main page")
            return
        
        # ëª¨ë“  ë¬¸ì„œ ë§í¬ ì¶”ì¶œ
        all_links = self.extract_article_links(soup, self.base_url)
        print(f"ğŸ“š Found {len(all_links)} links (articles + categories)")
        
        # ì¹´í…Œê³ ë¦¬ì™€ ë¬¸ì„œ ë¶„ë¦¬
        category_links = [link for link in all_links if link["type"] == "category"]
        article_links = [link for link in all_links if link["type"] == "article"]
        
        print(f"  â””â”€ {len(category_links)} categories, {len(article_links)} articles")
        
        # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ë¬¸ì„œ ë§í¬ ìˆ˜ì§‘
        for i, category_info in enumerate(category_links, 1):
            print(f"\nğŸ“‚ [{i}/{len(category_links)}] Exploring category: {category_info['title']}")
            category_soup = self.get_page_content(category_info["url"])
            if category_soup:
                category_articles = self.extract_article_links(
                    category_soup, category_info["url"]
                )
                # article íƒ€ì…ë§Œ ì¶”ê°€
                new_articles = [
                    link for link in category_articles if link["type"] == "article"
                ]
                # ì¹´í…Œê³ ë¦¬ëª… ì—…ë°ì´íŠ¸
                for article in new_articles:
                    article["category"] = category_info["title"]
                
                article_links.extend(new_articles)
                print(f"  â””â”€ Found {len(new_articles)} articles")
            
            time.sleep(self.delay)
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_articles = []
        for article in article_links:
            if article["url"] not in seen:
                unique_articles.append(article)
                seen.add(article["url"])
        
        article_links = unique_articles
        print(f"\nğŸ“„ Total unique articles to crawl: {len(article_links)}")
        
        # ê° ë¬¸ì„œ í¬ë¡¤ë§
        for i, article_info in enumerate(article_links, 1):
            print(f"\n[{i}/{len(article_links)}]")
            self.crawl_article(article_info)
        
        print(f"\nâœ¨ Crawling completed! Total pages crawled: {len(self.visited_urls)}")
        print(f"ğŸ“ Files saved to: {self.output_dir.absolute()}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = ChannelDocsCrawler(
        base_url="https://docs.channel.io/help/ko",
        output_dir="data/domain/saas/channel/docs",
        delay=1.0,  # 1ì´ˆ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
    )
    crawler.crawl()


if __name__ == "__main__":
    main()

