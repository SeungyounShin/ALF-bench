"""
ì±„ë„í†¡ ë¬¸ì„œ í¬ë¡¤ëŸ¬ V2 - ê³„ì¸µ êµ¬ì¡° ë°˜ì˜
https://docs.channel.io/help/ko ì˜ ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ì—¬ í¬ë¡¤ë§
"""

import json
import re
import time
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup, Tag
from markdownify import markdownify as md


class NavigationNode:
    """ë„¤ë¹„ê²Œì´ì…˜ íŠ¸ë¦¬ì˜ ë…¸ë“œ"""

    def __init__(self, title: str, url: Optional[str] = None, level: int = 0):
        self.title = title
        self.url = url
        self.level = level
        self.children: List[NavigationNode] = []
        self.parent: Optional[NavigationNode] = None

    def add_child(self, child: "NavigationNode"):
        child.parent = self
        self.children.append(child)

    def get_path(self) -> List[str]:
        """ë£¨íŠ¸ì—ì„œ í˜„ì¬ ë…¸ë“œê¹Œì§€ì˜ ê²½ë¡œ ë°˜í™˜"""
        path = []
        node = self
        while node.parent:
            path.insert(0, node.title)
            node = node.parent
        return path

    def __repr__(self):
        return f"<NavigationNode: {self.title} (level={self.level}, children={len(self.children)})>"


class ChannelDocsCrawlerV2:
    """ì±„ë„í†¡ ë¬¸ì„œ í¬ë¡¤ëŸ¬ V2 - ê³„ì¸µ êµ¬ì¡° ë°˜ì˜"""

    def __init__(
        self,
        base_url: str = "https://docs.channel.io/help/ko",
        output_dir: str = "data/domain/saas/channel/docs",
        delay: float = 1.0,
    ):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )
        self.visited_urls: Set[str] = set()
        self.navigation_tree: Optional[NavigationNode] = None

    def clean_category_name(self, text: str) -> str:
        """ì¹´í…Œê³ ë¦¬ëª…ì—ì„œ ì„¤ëª… ë¶€ë¶„ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ì¶”ì¶œ"""
        # ìˆ˜ë™ ë§¤í•‘
        mapping = {
            "ì±„ë„í†¡ ì‹œì‘í•˜ê¸°": "ì±„ë„í†¡_ì‹œì‘í•˜ê¸°",
            "ì±„ë„í†¡ ì„¤ì¹˜í•˜ê¸°": "ì±„ë„í†¡_ì„¤ì¹˜í•˜ê¸°",
            "ì±„ë„í†¡ í™œìš©í•˜ê¸°": "ì±„ë„í†¡_í™œìš©í•˜ê¸°",
            "ì±„ë„í†¡ êµ¬ë… ì´í•´í•˜ê¸°": "ì±„ë„í†¡_êµ¬ë…_ì´í•´í•˜ê¸°",
            "ì±„ë„ ì„¤ì •": "ì±„ë„_ì„¤ì •",
            "ê°œì¸ ì„¤ì •": "ê°œì¸_ì„¤ì •",
            "íŒ€ ë©”ì‹ ì €": "íŒ€_ë©”ì‹ ì €",
            "ê³ ê° ë©”ì‹ ì €": "ê³ ê°_ë©”ì‹ ì €",
            "ê³ ê° ì—°ë½ì²˜": "ê³ ê°_ì—°ë½ì²˜",
            "ì›Œí¬í”Œë¡œìš°": "ì›Œí¬í”Œë¡œìš°",
            "ì˜¤í¼ë ˆì´ì…˜": "ì˜¤í¼ë ˆì´ì…˜",
            "ë§ˆì¼€íŒ…": "ë§ˆì¼€íŒ…",
            "ì™¸ë¶€ì„œë¹„ìŠ¤ ì—°ë™": "ì™¸ë¶€ì„œë¹„ìŠ¤_ì—°ë™",
            "AI": "AI",
            "ì•±ìŠ¤í† ì–´": "ì•±ìŠ¤í† ì–´",
            "ë¯¸íŠ¸": "ë¯¸íŠ¸",
            "êµ¬ë… ë° ê²°ì œ": "êµ¬ë…_ë°_ê²°ì œ",
            "ë„íë¨¼íŠ¸": "ë„íë¨¼íŠ¸",
            "SLA": "SLA",
            "ì±„ë„ì—‘ìŠ¤": "ì±„ë„ì—‘ìŠ¤",
        }
        
        # ë§¤í•‘ì—ì„œ ì°¾ê¸°
        for key, value in mapping.items():
            if text.startswith(key):
                return value
        
        # ë§¤í•‘ì— ì—†ìœ¼ë©´ ê¸´ ì„¤ëª… ì œê±°
        # "XXXë‹¨ê³¨ì„ ë§Œë“œëŠ”..." -> "XXX"
        # "ê°œì˜ ì•„í‹°í´" ì œê±°
        text = re.sub(r"\d+ê°œì˜\s*ì•„í‹°í´.*$", "", text)
        
        # ì¡°ì‚¬ê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ë§Œ ì¶”ì¶œ (ì„/ë¥¼/ì˜/ëŠ”/ì´/ê°€/ì—/ë¡œ...)
        patterns = [
            r"^([ê°€-í£\s]+?)(ì„|ë¥¼|ì˜|ëŠ”|ì€|ì´|ê°€|ì™€|ê³¼|ì—|ë¡œ|ë¶€í„°|ê¹Œì§€|ì—ì„œ|ìœ¼ë¡œ|ë€,)",
            r"^([ê°€-í£\s]+?)\s*([ê°€-í£]+ì˜|[ê°€-í£]+ì€|[ê°€-í£]+ë¥¼)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                text = match.group(1).strip()
                break
        
        # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        text = re.sub(r"\s+", "_", text.strip())
        
        return text
    
    def sanitize_filename(self, text: str) -> str:
        """íŒŒì¼ëª…/í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬"""
        # ë¨¼ì € ì¹´í…Œê³ ë¦¬ëª… ì •ë¦¬ ì‹œë„
        text = self.clean_category_name(text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[<>:"/\\|?*]', "", text)
        
        if len(text) > 100:
            text = text[:100]
        return text

    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """URLì—ì„œ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.text, "lxml")
        except Exception as e:
            print(f"âŒ Failed to fetch {url}: {e}")
            return None

    def extract_navigation_structure(self, soup: BeautifulSoup) -> NavigationNode:
        """ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ì—¬ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë³€í™˜"""
        root = NavigationNode("root", level=-1)
        nav = soup.find("nav", class_="Navigation_nav__AfW5N")

        if not nav:
            print("âš ï¸  Navigation not found")
            return root

        # ëª¨ë“  OutlineItem ì°¾ê¸°
        items = nav.find_all("div", class_="OutlineItem_outline-item__zmH7D")
        
        stack = [root]  # í˜„ì¬ ë ˆë²¨ì˜ ë¶€ëª¨ ë…¸ë“œ ìŠ¤íƒ

        for item in items:
            # ì¸ë´íŠ¸ ë ˆë²¨ ì¶”ì¶œ
            style = item.get("style", "")
            indent_match = re.search(r"--b-outline-item-indent:\s*(\d+)px", style)
            indent = int(indent_match.group(1)) if indent_match else 0
            level = indent // 24  # 24px per level (ì¶”ì •)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_span = item.find("span", {"data-testid": "bezier-text"})
            if not text_span:
                continue

            title = text_span.get_text(strip=True)
            if not title:
                continue

            # URL ì¶”ì¶œ (ë§í¬ê°€ ìˆëŠ” ê²½ìš°)
            url = None
            link = item.find_parent("a", href=True)
            if link:
                url = urljoin(self.base_url, link["href"])

            # ë…¸ë“œ ìƒì„±
            node = NavigationNode(title, url, level)

            # ìŠ¤íƒì—ì„œ ì ì ˆí•œ ë¶€ëª¨ ì°¾ê¸°
            while len(stack) > 1 and stack[-1].level >= level:
                stack.pop()

            parent = stack[-1]
            parent.add_child(node)
            stack.append(node)

        return root

    def extract_navigation_structure_alt(self, soup: BeautifulSoup) -> NavigationNode:
        """ëŒ€ì²´ ë°©ë²•: í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡° ìƒì„±"""
        root = NavigationNode("root", level=-1)
        
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì™€ ë¬¸ì„œ ë§í¬ ì°¾ê¸°
        all_links = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if "/help/ko/" not in href:
                continue
            
            full_url = urljoin(self.base_url, href)
            title = link.get_text(strip=True)
            
            if title and full_url:
                is_category = "/categories/" in full_url
                is_article = "/articles/" in full_url
                
                if is_category or is_article:
                    all_links.append({
                        "title": title,
                        "url": full_url,
                        "type": "category" if is_category else "article"
                    })

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_links = []
        for link in all_links:
            key = (link["title"], link["url"])
            if key not in seen:
                seen.add(key)
                unique_links.append(link)

        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
        current_category = None
        for link in unique_links:
            if link["type"] == "category":
                # ìƒˆ ì¹´í…Œê³ ë¦¬ ë…¸ë“œ
                node = NavigationNode(link["title"], link["url"], level=0)
                root.add_child(node)
                current_category = node
            elif link["type"] == "article" and current_category:
                # í˜„ì¬ ì¹´í…Œê³ ë¦¬ì˜ í•˜ìœ„ ë¬¸ì„œ
                node = NavigationNode(link["title"], link["url"], level=1)
                current_category.add_child(node)
            elif link["type"] == "article":
                # ì¹´í…Œê³ ë¦¬ ì—†ëŠ” ìµœìƒìœ„ ë¬¸ì„œ
                node = NavigationNode(link["title"], link["url"], level=0)
                root.add_child(node)

        return root

    def print_navigation_tree(self, node: NavigationNode, indent: int = 0):
        """ë„¤ë¹„ê²Œì´ì…˜ íŠ¸ë¦¬ë¥¼ ì¶œë ¥ (ë””ë²„ê¹…ìš©)"""
        if node.title != "root":
            prefix = "  " * indent
            url_info = f" -> {node.url}" if node.url else ""
            print(f"{prefix}- {node.title}{url_info}")
        
        for child in node.children:
            self.print_navigation_tree(child, indent + 1)

    def convert_html_to_markdown(self, soup: BeautifulSoup) -> str:
        """HTML ì½˜í…ì¸ ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
        content_selectors = [
            ("article", {}),
            ("div", {"class": "article-content"}),
            ("main", {}),
        ]
        
        content = None
        for tag, attrs in content_selectors:
            content = soup.find(tag, attrs)
            if content:
                break
        
        if not content:
            content = soup.find("body")
        
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in content.find_all(["script", "style", "nav", "footer", "header"]):
            element.decompose()
        
        # Markdownìœ¼ë¡œ ë³€í™˜
        markdown_text = md(str(content), heading_style="ATX", bullets="*")
        markdown_text = re.sub(r"\n{3,}", "\n\n", markdown_text)
        
        return markdown_text.strip()

    def save_markdown(self, content: str, title: str, path: List[str], url: str):
        """ê³„ì¸µ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ì—¬ Markdown íŒŒì¼ë¡œ ì €ì¥"""
        # ê²½ë¡œ ìƒì„± (í´ë”ëª…ë§Œ ì •ë¦¬)
        dir_path = self.output_dir
        for folder in path[:-1]:  # ë§ˆì§€ë§‰ì€ íŒŒì¼ëª…
            cleaned_folder = self.sanitize_filename(folder)
            dir_path = dir_path / cleaned_folder
        
        dir_path.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (ì›ë³¸ ì œëª© ìœ ì§€, íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°)
        filename = re.sub(r'[<>:"/\\|?*]', "", title)
        filename = re.sub(r"\s+", "_", filename.strip())
        if len(filename) > 100:
            filename = filename[:100]
        filename = filename + ".md"
        
        filepath = dir_path / filename
        
        # Frontmatter ìƒì„±
        frontmatter = f"""---
title: {title}
path: {' > '.join(path)}
source_url: {url}
crawled_at: {time.strftime('%Y-%m-%d %H:%M:%S')}
---

"""
        
        # íŒŒì¼ ì €ì¥
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(frontmatter)
            f.write(f"# {title}\n\n")
            f.write(content)
        
        # ìƒëŒ€ ê²½ë¡œ ì¶œë ¥
        rel_path = filepath.relative_to(self.output_dir)
        print(f"âœ… Saved: {rel_path}")

    def crawl_node(self, node: NavigationNode):
        """ë…¸ë“œì˜ ë¬¸ì„œë¥¼ í¬ë¡¤ë§"""
        if not node.url:
            return
        
        if node.url in self.visited_urls:
            return
        
        self.visited_urls.add(node.url)
        
        # ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        path = node.get_path()
        
        print(f"ğŸ“„ Crawling: {' > '.join(path)}")
        
        # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.get_page_content(node.url)
        if not soup:
            return
        
        # ì œëª© ì¶”ì¶œ (h1 ìš°ì„ )
        title_tag = soup.find("h1")
        title = title_tag.get_text(strip=True) if title_tag else node.title
        
        # Markdown ë³€í™˜
        markdown_content = self.convert_html_to_markdown(soup)
        
        if markdown_content:
            self.save_markdown(markdown_content, title, path, node.url)
        
        time.sleep(self.delay)

    def expand_category_node(self, node: NavigationNode):
        """ì¹´í…Œê³ ë¦¬ ë…¸ë“œì˜ í•˜ìœ„ ë¬¸ì„œ ë° í•˜ìœ„ ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì•„ì„œ ì¶”ê°€"""
        if not node.url or "/categories/" not in node.url:
            return
        
        # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.get_page_content(node.url)
        if not soup:
            return
        
        # í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì™€ ë¬¸ì„œ ë§í¬ ì°¾ê¸°
        article_links = []
        subcategory_links = []
        
        for link in soup.find_all("a", href=True):
            href = link["href"]
            full_url = urljoin(self.base_url, href)
            title = link.get_text(strip=True)
            
            if not title or full_url in self.visited_urls:
                continue
            
            # í•˜ìœ„ ì¹´í…Œê³ ë¦¬ (categories ë§í¬)
            if "/categories/" in href and full_url != node.url:
                subcategory_links.append((title, full_url))
            # ë¬¸ì„œ (articles ë§í¬)
            elif "/articles/" in href:
                article_links.append((title, full_url))
        
        # ì¤‘ë³µ ì œê±° í•¨ìˆ˜
        def deduplicate(links):
            seen = set()
            unique = []
            for title, url in links:
                if url not in seen:
                    seen.add(url)
                    unique.append((title, url))
            return unique
        
        unique_subcategories = deduplicate(subcategory_links)
        unique_articles = deduplicate(article_links)
        
        # í•˜ìœ„ ì¹´í…Œê³ ë¦¬ë¥¼ ìì‹ ë…¸ë“œë¡œ ì¶”ê°€
        for title, url in unique_subcategories:
            child = NavigationNode(title, url, node.level + 1)
            node.add_child(child)
        
        # ë¬¸ì„œë¥¼ ìì‹ ë…¸ë“œë¡œ ì¶”ê°€
        for title, url in unique_articles:
            child = NavigationNode(title, url, node.level + 1)
            node.add_child(child)
        
        # ë°œê²¬ ë‚´ì—­ ì¶œë ¥
        if unique_subcategories or unique_articles:
            if unique_subcategories:
                print(f"  â””â”€ Found {len(unique_subcategories)} subcategories in: {node.title}")
            if unique_articles:
                print(f"  â””â”€ Found {len(unique_articles)} articles in: {node.title}")
        
        time.sleep(self.delay)

    def crawl_tree(self, node: NavigationNode):
        """íŠ¸ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ í¬ë¡¤ë§"""
        # ì¹´í…Œê³ ë¦¬ ë…¸ë“œë¼ë©´ í•˜ìœ„ ë¬¸ì„œë“¤ í™•ì¥
        if node.url and "/categories/" in node.url:
            self.expand_category_node(node)
        
        # í˜„ì¬ ë…¸ë“œ í¬ë¡¤ë§ (ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ëŠ” ìŠ¤í‚µ)
        if node.url and "/articles/" in node.url:
            self.crawl_node(node)
        
        # ìì‹ ë…¸ë“œë“¤ í¬ë¡¤ë§
        for child in node.children:
            self.crawl_tree(child)

    def crawl(self):
        """ì „ì²´ ë¬¸ì„œ í¬ë¡¤ë§ ì‹œì‘"""
        print(f"ğŸš€ Starting hierarchical crawl from {self.base_url}")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”ì¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("âŒ Failed to fetch main page")
            return
        
        # ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡° ì¶”ì¶œ
        print("\nğŸ“Š Extracting navigation structure...")
        self.navigation_tree = self.extract_navigation_structure_alt(soup)
        
        # êµ¬ì¡° ì¶œë ¥
        print("\nğŸ“‹ Navigation structure:")
        self.print_navigation_tree(self.navigation_tree)
        
        # ì¹´ìš´íŠ¸
        def count_nodes(node):
            count = 1 if node.url else 0
            for child in node.children:
                count += count_nodes(child)
            return count
        
        total_pages = count_nodes(self.navigation_tree)
        print(f"\nğŸ“š Found {total_pages} pages to crawl\n")
        
        # í¬ë¡¤ë§ ì‹œì‘
        self.crawl_tree(self.navigation_tree)
        
        print(f"\nâœ¨ Crawling completed! Total pages crawled: {len(self.visited_urls)}")
        print(f"ğŸ“ Files saved to: {self.output_dir.absolute()}")
        
        # êµ¬ì¡°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        self.save_structure_json()

    def save_structure_json(self):
        """ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        
        def node_to_dict(node: NavigationNode) -> Dict:
            return {
                "title": node.title,
                "url": node.url,
                "level": node.level,
                "children": [node_to_dict(child) for child in node.children]
            }
        
        structure = node_to_dict(self.navigation_tree)
        
        json_path = self.output_dir.parent / "structure.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(structure, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ Structure saved to: {json_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = ChannelDocsCrawlerV2(
        base_url="https://docs.channel.io/help/ko",
        output_dir="data/domain/saas/channel/docs",
        delay=1.0,
    )
    crawler.crawl()


if __name__ == "__main__":
    main()

